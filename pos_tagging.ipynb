{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(text_file):\n",
    "\n",
    "    with open(text_file, encoding=\"utf-8\") as file:\n",
    "        sentences = []\n",
    "        for line in file:\n",
    "            if line != '\\n':\n",
    "                tuples = []\n",
    "                for word in line.split():\n",
    "                    w, tag = word.split('_')\n",
    "                    tuples.append((w,tag))            \n",
    "                sentences.append(tuples)\n",
    "        \n",
    "        return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(text_file=\"data/macmorpho-v3/macmorpho-train.txt\")\n",
    "test_data = load_data(text_file=\"data/macmorpho-v3/macmorpho-test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados de treinamento\n",
      "Número de frases: 37948\n",
      "Tamanho do vocabulário: 52759\n",
      "Número de tags: 26\n"
     ]
    }
   ],
   "source": [
    "num_words = 0\n",
    "num_tags = 0\n",
    "words_set = set()\n",
    "tags_set = set()\n",
    "tag_count = {}\n",
    "\n",
    "for sentence in train_data:\n",
    "    for word, tag in sentence:\n",
    "        words_set.add(word)\n",
    "        tags_set.add(tag)\n",
    "        \n",
    "        if tag in tag_count:\n",
    "            tag_count[tag] += 1\n",
    "        else:\n",
    "            tag_count[tag] = 1\n",
    "\n",
    "print(\"Dados de treinamento\")\n",
    "print(f\"Número de frases: {len(train_data)}\")\n",
    "print(f\"Tamanho do vocabulário: {len(words_set)}\")\n",
    "print(f\"Número de tags: {len(tags_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tags = list(tag_count.keys())\n",
    "counts = list(tag_count.values())\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(tags, counts)\n",
    "plt.xlabel('Tags')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Histogram of Tag Counts')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()  # Adjust layout for better fit\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados de teste\n",
      "Número de frases: 9987\n",
      "Tamanho do vocabulário: 25140\n",
      "Número de tags: 26\n"
     ]
    }
   ],
   "source": [
    "num_words = 0\n",
    "num_tags = 0\n",
    "words_set = set()\n",
    "tags_set = set()\n",
    "\n",
    "\n",
    "for sentence in test_data:\n",
    "    for word, tag in sentence:\n",
    "        words_set.add(word)\n",
    "        tags_set.add(tag)\n",
    "\n",
    "print(\"Dados de teste\")\n",
    "print(f\"Número de frases: {len(test_data)}\")\n",
    "print(f\"Tamanho do vocabulário: {len(words_set)}\")\n",
    "print(f\"Número de tags: {len(tags_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='google-bert/bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", torch_dtype=torch.float16)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags.append(\"<pad>\")\n",
    "\n",
    "tag2idx = {tag:idx for idx, tag in enumerate(tags)}\n",
    "idx2tag = {idx:tag for idx, tag in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosDataset(Dataset):\n",
    "\n",
    "    \"\"\"A custom dataset class for POS tagging using BERT tokenizer\"\"\" \n",
    "\n",
    "    def __init__(self, tagged_sentences, tokenizer: BertTokenizer, tag2idx, idx2tag):\n",
    "        \"\"\"Initializes the PosDataset instance.\"\"\" \n",
    "        \n",
    "        sentences = []\n",
    "        tags= []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag2idx = tag2idx\n",
    "        self.idx2tag = idx2tag\n",
    "        self.max_seqlen = 0\n",
    "\n",
    "        for sentence in tagged_sentences:\n",
    "            words = [word_tag_tuple[0] for word_tag_tuple in sentence]\n",
    "            sentence_tags = [word_tag_tuple[1] for word_tag_tuple in sentence]\n",
    "            sentences.append([\"[CLS]\"] + words + [\"[SEP]\"])\n",
    "            tags.append([\"<pad>\"] + sentence_tags + [\"<pad>\"])\n",
    "\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        words, tags = self.sentences[idx], self.tags[idx]\n",
    "        \n",
    "        X, y = [], []\n",
    "\n",
    "        word_head_masks = []\n",
    "        for w, t in zip(words, tags):\n",
    "\n",
    "            if w not in [\"[CLS]\", \"[SEP]\"]:\n",
    "                tokens = self.tokenizer.tokenize(w)\n",
    "            else:\n",
    "                tokens = [w]\n",
    "            \n",
    "            word_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "            \n",
    "            word_head_mask = [1] + [0]*(len(tokens) - 1)\n",
    "            t = [t] + [\"<pad>\"] * (len(tokens) - 1) \n",
    "\n",
    "            tags_idx = [self.tag2idx[tag] for tag in t]\n",
    "\n",
    "            X.extend(word_ids)\n",
    "            word_head_masks.extend(word_head_mask)\n",
    "            y.extend(tags_idx)\n",
    "\n",
    "        seqlen = len(y)\n",
    "\n",
    "        words = \" \".join(words)\n",
    "        tags = \" \".join(tags)\n",
    "        return words, X, word_head_masks, tags, y, seqlen\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_dataset = PosDataset(train_data, tokenizer, tag2idx, idx2tag)\n",
    "pos_test_dataset = PosDataset(test_data, tokenizer, tag2idx, idx2tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Pytorch Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_function(batch):\n",
    "\n",
    "    words = [sample[0] for sample in batch]\n",
    "    is_heads = [sample[2] for sample in batch]\n",
    "    tags = [sample[3] for sample in batch]\n",
    "    seqlens = [sample[-1] for sample in batch]\n",
    "\n",
    "    maxlen = max(seqlens)\n",
    "\n",
    "    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch]\n",
    "    x = f(1, maxlen)\n",
    "    y = f(-2, maxlen)\n",
    "\n",
    "    return words, torch.LongTensor(x), is_heads, tags, torch.LongTensor(y), seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=pos_train_dataset,\n",
    "                            batch_size=8,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4,\n",
    "                            collate_fn=collate_function)\n",
    "test_dataloader = DataLoader(dataset=pos_test_dataset,\n",
    "                            batch_size=8,\n",
    "                            shuffle=False,\n",
    "                            num_workers=4,\n",
    "                            collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Number of train batches\",len(train_dataloader))\n",
    "print( \"Number of test batches\",len(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTunedBert(nn.Module):\n",
    "    def __init__(self, num_labels = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "        self.fc1 = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.training:\n",
    "            self.bert.train()\n",
    "            outputs = self.bert(x)\n",
    "            out = outputs.last_hidden_state\n",
    "        \n",
    "        else:\n",
    "            self.bert.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = self.bert(x)\n",
    "                out = outputs.last_hidden_state\n",
    "\n",
    "        \n",
    "        logits = self.fc1(out)\n",
    "        pred = logits.argmax(-1)\n",
    "\n",
    "        return logits, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FineTunedBert(num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTunedBert(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=768, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 3.4504833221435547\n",
      "Step: 10, Loss: 0.8280138373374939\n",
      "Step: 20, Loss: 0.6062771677970886\n",
      "Step: 30, Loss: 0.5226663947105408\n",
      "Step: 40, Loss: 0.24298302829265594\n",
      "Step: 50, Loss: 0.299858421087265\n",
      "Step: 60, Loss: 0.29446738958358765\n",
      "Step: 70, Loss: 0.24812856316566467\n",
      "Step: 80, Loss: 0.20825053751468658\n",
      "Step: 90, Loss: 0.25187522172927856\n",
      "Step: 100, Loss: 0.14485889673233032\n",
      "Step: 110, Loss: 0.18319600820541382\n",
      "Step: 120, Loss: 0.19505088031291962\n",
      "Step: 130, Loss: 0.17247146368026733\n",
      "Step: 140, Loss: 0.1575685441493988\n",
      "Step: 150, Loss: 0.14485695958137512\n",
      "Step: 160, Loss: 0.19753888249397278\n",
      "Step: 170, Loss: 0.1701732873916626\n",
      "Step: 180, Loss: 0.17427660524845123\n",
      "Step: 190, Loss: 0.11922323703765869\n",
      "Step: 200, Loss: 0.15020260214805603\n",
      "Step: 210, Loss: 0.1068076342344284\n",
      "Step: 220, Loss: 0.1459614634513855\n",
      "Step: 230, Loss: 0.16925950348377228\n",
      "Step: 240, Loss: 0.1395181268453598\n",
      "Step: 250, Loss: 0.0890178456902504\n",
      "Step: 260, Loss: 0.08612261712551117\n",
      "Step: 270, Loss: 0.1599344164133072\n",
      "Step: 280, Loss: 0.09864937514066696\n",
      "Step: 290, Loss: 0.24893449246883392\n",
      "Step: 300, Loss: 0.19544291496276855\n",
      "Step: 310, Loss: 0.157590851187706\n",
      "Step: 320, Loss: 0.13983650505542755\n",
      "Step: 330, Loss: 0.09063879400491714\n",
      "Step: 340, Loss: 0.05589170381426811\n",
      "Step: 350, Loss: 0.1545792669057846\n",
      "Step: 360, Loss: 0.07677459716796875\n",
      "Step: 370, Loss: 0.16784590482711792\n",
      "Step: 380, Loss: 0.10497202724218369\n",
      "Step: 390, Loss: 0.16606806218624115\n",
      "Step: 400, Loss: 0.10848376899957657\n",
      "Step: 410, Loss: 0.04874664545059204\n",
      "Step: 420, Loss: 0.14104527235031128\n",
      "Step: 430, Loss: 0.09802497923374176\n",
      "Step: 440, Loss: 0.10981225222349167\n",
      "Step: 450, Loss: 0.04554338380694389\n",
      "Step: 460, Loss: 0.11601651459932327\n",
      "Step: 470, Loss: 0.09918349981307983\n",
      "Step: 480, Loss: 0.13485726714134216\n",
      "Step: 490, Loss: 0.08510970324277878\n",
      "Step: 500, Loss: 0.05399104580283165\n",
      "Step: 510, Loss: 0.07925086468458176\n",
      "Step: 520, Loss: 0.08738213032484055\n",
      "Step: 530, Loss: 0.10771183669567108\n",
      "Step: 540, Loss: 0.09283793717622757\n",
      "Step: 550, Loss: 0.0866694450378418\n",
      "Step: 560, Loss: 0.07858534157276154\n",
      "Step: 570, Loss: 0.07634786516427994\n",
      "Step: 580, Loss: 0.1087600439786911\n",
      "Step: 590, Loss: 0.1617860049009323\n",
      "Step: 600, Loss: 0.10114061832427979\n",
      "Step: 610, Loss: 0.1269102841615677\n",
      "Step: 620, Loss: 0.12072458863258362\n",
      "Step: 630, Loss: 0.08467763662338257\n",
      "Step: 640, Loss: 0.11071435362100601\n",
      "Step: 650, Loss: 0.09308523684740067\n",
      "Step: 660, Loss: 0.08292383700609207\n",
      "Step: 670, Loss: 0.12289868295192719\n",
      "Step: 680, Loss: 0.11490201205015182\n",
      "Step: 690, Loss: 0.06306710839271545\n",
      "Step: 700, Loss: 0.08465114235877991\n",
      "Step: 710, Loss: 0.08198661357164383\n",
      "Step: 720, Loss: 0.09189274162054062\n",
      "Step: 730, Loss: 0.04054994136095047\n",
      "Step: 740, Loss: 0.10433103144168854\n",
      "Step: 750, Loss: 0.11387860029935837\n",
      "Step: 760, Loss: 0.07599058747291565\n",
      "Step: 770, Loss: 0.0967739149928093\n",
      "Step: 780, Loss: 0.0707094594836235\n",
      "Step: 790, Loss: 0.06995166838169098\n",
      "Step: 800, Loss: 0.07608602941036224\n",
      "Step: 810, Loss: 0.13963469862937927\n",
      "Step: 820, Loss: 0.09036333113908768\n",
      "Step: 830, Loss: 0.07158426195383072\n",
      "Step: 840, Loss: 0.07495520263910294\n",
      "Step: 850, Loss: 0.06524515897035599\n",
      "Step: 860, Loss: 0.09828348457813263\n",
      "Step: 870, Loss: 0.04302423447370529\n",
      "Step: 880, Loss: 0.04615512862801552\n",
      "Step: 890, Loss: 0.09955493360757828\n",
      "Step: 900, Loss: 0.08232948929071426\n",
      "Step: 910, Loss: 0.049745772033929825\n",
      "Step: 920, Loss: 0.058219581842422485\n",
      "Step: 930, Loss: 0.09706736356019974\n",
      "Step: 940, Loss: 0.09424615651369095\n",
      "Step: 950, Loss: 0.13173508644104004\n",
      "Step: 960, Loss: 0.14848078787326813\n",
      "Step: 970, Loss: 0.05111250653862953\n",
      "Step: 980, Loss: 0.05815783143043518\n",
      "Step: 990, Loss: 0.06807057559490204\n",
      "Step: 1000, Loss: 0.039176009595394135\n",
      "Step: 1010, Loss: 0.08176971226930618\n",
      "Step: 1020, Loss: 0.09086445719003677\n",
      "Step: 1030, Loss: 0.11833637207746506\n",
      "Step: 1040, Loss: 0.04487413167953491\n",
      "Step: 1050, Loss: 0.0584888719022274\n",
      "Step: 1060, Loss: 0.07985635101795197\n",
      "Step: 1070, Loss: 0.037731632590293884\n",
      "Step: 1080, Loss: 0.07913238555192947\n",
      "Step: 1090, Loss: 0.05296441912651062\n",
      "Step: 1100, Loss: 0.05533437058329582\n",
      "Step: 1110, Loss: 0.07573740929365158\n",
      "Step: 1120, Loss: 0.0679846853017807\n",
      "Step: 1130, Loss: 0.08417746424674988\n",
      "Step: 1140, Loss: 0.08446952700614929\n",
      "Step: 1150, Loss: 0.07583319395780563\n",
      "Step: 1160, Loss: 0.11655250936746597\n",
      "Step: 1170, Loss: 0.08923694491386414\n",
      "Step: 1180, Loss: 0.0572332888841629\n",
      "Step: 1190, Loss: 0.07552654296159744\n",
      "Step: 1200, Loss: 0.06077483668923378\n",
      "Step: 1210, Loss: 0.10916152596473694\n",
      "Step: 1220, Loss: 0.10733868181705475\n",
      "Step: 1230, Loss: 0.024373453110456467\n",
      "Step: 1240, Loss: 0.03992791846394539\n",
      "Step: 1250, Loss: 0.08183072507381439\n",
      "Step: 1260, Loss: 0.03302265331149101\n",
      "Step: 1270, Loss: 0.04541449248790741\n",
      "Step: 1280, Loss: 0.10285556316375732\n",
      "Step: 1290, Loss: 0.05180765315890312\n",
      "Step: 1300, Loss: 0.05098431929945946\n",
      "Step: 1310, Loss: 0.07507610321044922\n",
      "Step: 1320, Loss: 0.06901680678129196\n",
      "Step: 1330, Loss: 0.05444791913032532\n",
      "Step: 1340, Loss: 0.035729143768548965\n",
      "Step: 1350, Loss: 0.19742903113365173\n",
      "Step: 1360, Loss: 0.04993334040045738\n",
      "Step: 1370, Loss: 0.043813738971948624\n",
      "Step: 1380, Loss: 0.06410345435142517\n",
      "Step: 1390, Loss: 0.05616651102900505\n",
      "Step: 1400, Loss: 0.0750754326581955\n",
      "Step: 1410, Loss: 0.09388155490159988\n",
      "Step: 1420, Loss: 0.03152542933821678\n",
      "Step: 1430, Loss: 0.038280047476291656\n",
      "Step: 1440, Loss: 0.060378868132829666\n",
      "Step: 1450, Loss: 0.07932357490062714\n",
      "Step: 1460, Loss: 0.049653179943561554\n",
      "Step: 1470, Loss: 0.05451050028204918\n",
      "Step: 1480, Loss: 0.04508589580655098\n",
      "Step: 1490, Loss: 0.029601402580738068\n",
      "Step: 1500, Loss: 0.09675450623035431\n",
      "Step: 1510, Loss: 0.05479970946907997\n",
      "Step: 1520, Loss: 0.08811626583337784\n",
      "Step: 1530, Loss: 0.13222970068454742\n",
      "Step: 1540, Loss: 0.05526125803589821\n",
      "Step: 1550, Loss: 0.09596645832061768\n",
      "Step: 1560, Loss: 0.0648292824625969\n",
      "Step: 1570, Loss: 0.059017159044742584\n",
      "Step: 1580, Loss: 0.05992163345217705\n",
      "Step: 1590, Loss: 0.050244614481925964\n",
      "Step: 1600, Loss: 0.05716778337955475\n",
      "Step: 1610, Loss: 0.0548560805618763\n",
      "Step: 1620, Loss: 0.05088658258318901\n",
      "Step: 1630, Loss: 0.0556674599647522\n",
      "Step: 1640, Loss: 0.0545082725584507\n",
      "Step: 1650, Loss: 0.03328194096684456\n",
      "Step: 1660, Loss: 0.04587845131754875\n",
      "Step: 1670, Loss: 0.05210272967815399\n",
      "Step: 1680, Loss: 0.06991736590862274\n",
      "Step: 1690, Loss: 0.03900972753763199\n",
      "Step: 1700, Loss: 0.09359829127788544\n",
      "Step: 1710, Loss: 0.03462854400277138\n",
      "Step: 1720, Loss: 0.04926538094878197\n",
      "Step: 1730, Loss: 0.025545407086610794\n",
      "Step: 1740, Loss: 0.06789091229438782\n",
      "Step: 1750, Loss: 0.07438750565052032\n",
      "Step: 1760, Loss: 0.05932563915848732\n",
      "Step: 1770, Loss: 0.03885224834084511\n",
      "Step: 1780, Loss: 0.024179257452487946\n",
      "Step: 1790, Loss: 0.06405006349086761\n",
      "Step: 1800, Loss: 0.06981530040502548\n",
      "Step: 1810, Loss: 0.07218274474143982\n",
      "Step: 1820, Loss: 0.04019812121987343\n",
      "Step: 1830, Loss: 0.05290200933814049\n",
      "Step: 1840, Loss: 0.05095389485359192\n",
      "Step: 1850, Loss: 0.06901159882545471\n",
      "Step: 1860, Loss: 0.06940935552120209\n",
      "Step: 1870, Loss: 0.026642125099897385\n",
      "Step: 1880, Loss: 0.03618265315890312\n",
      "Step: 1890, Loss: 0.06124802306294441\n",
      "Step: 1900, Loss: 0.04014872759580612\n",
      "Step: 1910, Loss: 0.0401340089738369\n",
      "Step: 1920, Loss: 0.06393874436616898\n",
      "Step: 1930, Loss: 0.05532411113381386\n",
      "Step: 1940, Loss: 0.07599089294672012\n",
      "Step: 1950, Loss: 0.04003744199872017\n",
      "Step: 1960, Loss: 0.049438584595918655\n",
      "Step: 1970, Loss: 0.07673459500074387\n",
      "Step: 1980, Loss: 0.033130332827568054\n",
      "Step: 1990, Loss: 0.04265444725751877\n",
      "Step: 2000, Loss: 0.07607394456863403\n",
      "Step: 2010, Loss: 0.035040922462940216\n",
      "Step: 2020, Loss: 0.05805592983961105\n",
      "Step: 2030, Loss: 0.048111364245414734\n",
      "Step: 2040, Loss: 0.04115942120552063\n",
      "Step: 2050, Loss: 0.04276496544480324\n",
      "Step: 2060, Loss: 0.032889436930418015\n",
      "Step: 2070, Loss: 0.05796719714999199\n",
      "Step: 2080, Loss: 0.048128679394721985\n",
      "Step: 2090, Loss: 0.04207199066877365\n",
      "Step: 2100, Loss: 0.0357198491692543\n",
      "Step: 2110, Loss: 0.0778573676943779\n",
      "Step: 2120, Loss: 0.06534390151500702\n",
      "Step: 2130, Loss: 0.019784925505518913\n",
      "Step: 2140, Loss: 0.06437636911869049\n",
      "Step: 2150, Loss: 0.04710245877504349\n",
      "Step: 2160, Loss: 0.04096101224422455\n",
      "Step: 2170, Loss: 0.09634173661470413\n",
      "Step: 2180, Loss: 0.05198643356561661\n",
      "Step: 2190, Loss: 0.08200480043888092\n",
      "Step: 2200, Loss: 0.059702083468437195\n",
      "Step: 2210, Loss: 0.045942891389131546\n",
      "Step: 2220, Loss: 0.12235192954540253\n",
      "Step: 2230, Loss: 0.07181300967931747\n",
      "Step: 2240, Loss: 0.05827820673584938\n",
      "Step: 2250, Loss: 0.048144202679395676\n",
      "Step: 2260, Loss: 0.03673539683222771\n",
      "Step: 2270, Loss: 0.08512386679649353\n",
      "Step: 2280, Loss: 0.16139303147792816\n",
      "Step: 2290, Loss: 0.050176844000816345\n",
      "Step: 2300, Loss: 0.10734666883945465\n",
      "Step: 2310, Loss: 0.09343530982732773\n",
      "Step: 2320, Loss: 0.0485556535422802\n",
      "Step: 2330, Loss: 0.06432171165943146\n",
      "Step: 2340, Loss: 0.08181742578744888\n",
      "Step: 2350, Loss: 0.04206099361181259\n",
      "Step: 2360, Loss: 0.04882322624325752\n",
      "Step: 2370, Loss: 0.044690463691949844\n",
      "Step: 2380, Loss: 0.12693211436271667\n",
      "Step: 2390, Loss: 0.04967016354203224\n",
      "Step: 2400, Loss: 0.06363677978515625\n",
      "Step: 2410, Loss: 0.09652092307806015\n",
      "Step: 2420, Loss: 0.028402872383594513\n",
      "Step: 2430, Loss: 0.06180557236075401\n",
      "Step: 2440, Loss: 0.06047093868255615\n",
      "Step: 2450, Loss: 0.04335380345582962\n",
      "Step: 2460, Loss: 0.03429793193936348\n",
      "Step: 2470, Loss: 0.06142690032720566\n",
      "Step: 2480, Loss: 0.07853620499372482\n",
      "Step: 2490, Loss: 0.07229875028133392\n",
      "Step: 2500, Loss: 0.08852890878915787\n",
      "Step: 2510, Loss: 0.08126336336135864\n",
      "Step: 2520, Loss: 0.07744789868593216\n",
      "Step: 2530, Loss: 0.10891909152269363\n",
      "Step: 2540, Loss: 0.04397280886769295\n",
      "Step: 2550, Loss: 0.017405763268470764\n",
      "Step: 2560, Loss: 0.0544898621737957\n",
      "Step: 2570, Loss: 0.08209966868162155\n",
      "Step: 2580, Loss: 0.07718273252248764\n",
      "Step: 2590, Loss: 0.04625088721513748\n",
      "Step: 2600, Loss: 0.07611100375652313\n",
      "Step: 2610, Loss: 0.049806926399469376\n",
      "Step: 2620, Loss: 0.07793727517127991\n",
      "Step: 2630, Loss: 0.07894667237997055\n",
      "Step: 2640, Loss: 0.07522134482860565\n",
      "Step: 2650, Loss: 0.061105430126190186\n",
      "Step: 2660, Loss: 0.05680262669920921\n",
      "Step: 2670, Loss: 0.03189340978860855\n",
      "Step: 2680, Loss: 0.04805302619934082\n",
      "Step: 2690, Loss: 0.12213857471942902\n",
      "Step: 2700, Loss: 0.062410544604063034\n",
      "Step: 2710, Loss: 0.040427640080451965\n",
      "Step: 2720, Loss: 0.04329245537519455\n",
      "Step: 2730, Loss: 0.05826674401760101\n",
      "Step: 2740, Loss: 0.06075757369399071\n",
      "Step: 2750, Loss: 0.06577365845441818\n",
      "Step: 2760, Loss: 0.014615538530051708\n",
      "Step: 2770, Loss: 0.08524350076913834\n",
      "Step: 2780, Loss: 0.01228482834994793\n",
      "Step: 2790, Loss: 0.038566119968891144\n",
      "Step: 2800, Loss: 0.043777719140052795\n",
      "Step: 2810, Loss: 0.03654910624027252\n",
      "Step: 2820, Loss: 0.037528544664382935\n",
      "Step: 2830, Loss: 0.07205991446971893\n",
      "Step: 2840, Loss: 0.0823434516787529\n",
      "Step: 2850, Loss: 0.03402062505483627\n",
      "Step: 2860, Loss: 0.0324416384100914\n",
      "Step: 2870, Loss: 0.06360545009374619\n",
      "Step: 2880, Loss: 0.10538182407617569\n",
      "Step: 2890, Loss: 0.01617426984012127\n",
      "Step: 2900, Loss: 0.0494515523314476\n",
      "Step: 2910, Loss: 0.04477623105049133\n",
      "Step: 2920, Loss: 0.05533754453063011\n",
      "Step: 2930, Loss: 0.054584965109825134\n",
      "Step: 2940, Loss: 0.028607385233044624\n",
      "Step: 2950, Loss: 0.05425100401043892\n",
      "Step: 2960, Loss: 0.0837760716676712\n",
      "Step: 2970, Loss: 0.05714838579297066\n",
      "Step: 2980, Loss: 0.03384862095117569\n",
      "Step: 2990, Loss: 0.05863262712955475\n",
      "Step: 3000, Loss: 0.02456953190267086\n",
      "Step: 3010, Loss: 0.04291413724422455\n",
      "Step: 3020, Loss: 0.22697259485721588\n",
      "Step: 3030, Loss: 0.029167691245675087\n",
      "Step: 3040, Loss: 0.0451534166932106\n",
      "Step: 3050, Loss: 0.06004897877573967\n",
      "Step: 3060, Loss: 0.03738611564040184\n",
      "Step: 3070, Loss: 0.0692816972732544\n",
      "Step: 3080, Loss: 0.022258026525378227\n",
      "Step: 3090, Loss: 0.01508252415806055\n",
      "Step: 3100, Loss: 0.05270552262663841\n",
      "Step: 3110, Loss: 0.03956299275159836\n",
      "Step: 3120, Loss: 0.031954001635313034\n",
      "Step: 3130, Loss: 0.024291444569826126\n",
      "Step: 3140, Loss: 0.042742714285850525\n",
      "Step: 3150, Loss: 0.0950787290930748\n",
      "Step: 3160, Loss: 0.0314193032681942\n",
      "Step: 3170, Loss: 0.015357407741248608\n",
      "Step: 3180, Loss: 0.08745209872722626\n",
      "Step: 3190, Loss: 0.038019564002752304\n",
      "Step: 3200, Loss: 0.0806439146399498\n",
      "Step: 3210, Loss: 0.11089041829109192\n",
      "Step: 3220, Loss: 0.04226166754961014\n",
      "Step: 3230, Loss: 0.03517654165625572\n",
      "Step: 3240, Loss: 0.12010771781206131\n",
      "Step: 3250, Loss: 0.04516223818063736\n",
      "Step: 3260, Loss: 0.037088289856910706\n",
      "Step: 3270, Loss: 0.03922272473573685\n",
      "Step: 3280, Loss: 0.01746787689626217\n",
      "Step: 3290, Loss: 0.05612241476774216\n",
      "Step: 3300, Loss: 0.054461997002363205\n",
      "Step: 3310, Loss: 0.08056135475635529\n",
      "Step: 3320, Loss: 0.0683077946305275\n",
      "Step: 3330, Loss: 0.051547788083553314\n",
      "Step: 3340, Loss: 0.04465709626674652\n",
      "Step: 3350, Loss: 0.06568654626607895\n",
      "Step: 3360, Loss: 0.03668028116226196\n",
      "Step: 3370, Loss: 0.11695131659507751\n",
      "Step: 3380, Loss: 0.07026425749063492\n",
      "Step: 3390, Loss: 0.043488577008247375\n",
      "Step: 3400, Loss: 0.06214368715882301\n",
      "Step: 3410, Loss: 0.06191067770123482\n",
      "Step: 3420, Loss: 0.02299952693283558\n",
      "Step: 3430, Loss: 0.09434323757886887\n",
      "Step: 3440, Loss: 0.034818314015865326\n",
      "Step: 3450, Loss: 0.021554436534643173\n",
      "Step: 3460, Loss: 0.027515292167663574\n",
      "Step: 3470, Loss: 0.08583743125200272\n",
      "Step: 3480, Loss: 0.08169985562562943\n",
      "Step: 3490, Loss: 0.05229712277650833\n",
      "Step: 3500, Loss: 0.015954477712512016\n",
      "Step: 3510, Loss: 0.043837498873472214\n",
      "Step: 3520, Loss: 0.03488922864198685\n",
      "Step: 3530, Loss: 0.061961449682712555\n",
      "Step: 3540, Loss: 0.05152749642729759\n",
      "Step: 3550, Loss: 0.0680321604013443\n",
      "Step: 3560, Loss: 0.05536867305636406\n",
      "Step: 3570, Loss: 0.050475019961595535\n",
      "Step: 3580, Loss: 0.018692437559366226\n",
      "Step: 3590, Loss: 0.029998812824487686\n",
      "Step: 3600, Loss: 0.10004778206348419\n",
      "Step: 3610, Loss: 0.09995122253894806\n",
      "Step: 3620, Loss: 0.03407453000545502\n",
      "Step: 3630, Loss: 0.06644855439662933\n",
      "Step: 3640, Loss: 0.04629506915807724\n",
      "Step: 3650, Loss: 0.018703535199165344\n",
      "Step: 3660, Loss: 0.05087970942258835\n",
      "Step: 3670, Loss: 0.07701809704303741\n",
      "Step: 3680, Loss: 0.07343071699142456\n",
      "Step: 3690, Loss: 0.033481281250715256\n",
      "Step: 3700, Loss: 0.06489094346761703\n",
      "Step: 3710, Loss: 0.045909199863672256\n",
      "Step: 3720, Loss: 0.11885074526071548\n",
      "Step: 3730, Loss: 0.03984633833169937\n",
      "Step: 3740, Loss: 0.06258106976747513\n",
      "Step: 3750, Loss: 0.09804446250200272\n",
      "Step: 3760, Loss: 0.07130700349807739\n",
      "Step: 3770, Loss: 0.03576792776584625\n",
      "Step: 3780, Loss: 0.1460188776254654\n",
      "Step: 3790, Loss: 0.12554873526096344\n",
      "Step: 3800, Loss: 0.06264884024858475\n",
      "Step: 3810, Loss: 0.03154708817601204\n",
      "Step: 3820, Loss: 0.05111643299460411\n",
      "Step: 3830, Loss: 0.05593661591410637\n",
      "Step: 3840, Loss: 0.07110506296157837\n",
      "Step: 3850, Loss: 0.08347117155790329\n",
      "Step: 3860, Loss: 0.08672251552343369\n",
      "Step: 3870, Loss: 0.07797975093126297\n",
      "Step: 3880, Loss: 0.08958198875188828\n",
      "Step: 3890, Loss: 0.07894254475831985\n",
      "Step: 3900, Loss: 0.10369021445512772\n",
      "Step: 3910, Loss: 0.07676628977060318\n",
      "Step: 3920, Loss: 0.04365403577685356\n",
      "Step: 3930, Loss: 0.02421298436820507\n",
      "Step: 3940, Loss: 0.06301278620958328\n",
      "Step: 3950, Loss: 0.033437419682741165\n",
      "Step: 3960, Loss: 0.031193558126688004\n",
      "Step: 3970, Loss: 0.055038273334503174\n",
      "Step: 3980, Loss: 0.018007829785346985\n",
      "Step: 3990, Loss: 0.021135451272130013\n",
      "Step: 4000, Loss: 0.04076230898499489\n",
      "Step: 4010, Loss: 0.044837579131126404\n",
      "Step: 4020, Loss: 0.07658257335424423\n",
      "Step: 4030, Loss: 0.056981950998306274\n",
      "Step: 4040, Loss: 0.09055031836032867\n",
      "Step: 4050, Loss: 0.0570058636367321\n",
      "Step: 4060, Loss: 0.03384600207209587\n",
      "Step: 4070, Loss: 0.025993207469582558\n",
      "Step: 4080, Loss: 0.030112896114587784\n",
      "Step: 4090, Loss: 0.053863801062107086\n",
      "Step: 4100, Loss: 0.04116593301296234\n",
      "Step: 4110, Loss: 0.06268172711133957\n",
      "Step: 4120, Loss: 0.07204905152320862\n",
      "Step: 4130, Loss: 0.07351011782884598\n",
      "Step: 4140, Loss: 0.04355161264538765\n",
      "Step: 4150, Loss: 0.05115335434675217\n",
      "Step: 4160, Loss: 0.03847038373351097\n",
      "Step: 4170, Loss: 0.0802343562245369\n",
      "Step: 4180, Loss: 0.044031888246536255\n",
      "Step: 4190, Loss: 0.07494193315505981\n",
      "Step: 4200, Loss: 0.04360131919384003\n",
      "Step: 4210, Loss: 0.08722975105047226\n",
      "Step: 4220, Loss: 0.1124894917011261\n",
      "Step: 4230, Loss: 0.045623913407325745\n",
      "Step: 4240, Loss: 0.05906033143401146\n",
      "Step: 4250, Loss: 0.02230752818286419\n",
      "Step: 4260, Loss: 0.07576784491539001\n",
      "Step: 4270, Loss: 0.06351446360349655\n",
      "Step: 4280, Loss: 0.02878463640809059\n",
      "Step: 4290, Loss: 0.048175692558288574\n",
      "Step: 4300, Loss: 0.027270272374153137\n",
      "Step: 4310, Loss: 0.04119182750582695\n",
      "Step: 4320, Loss: 0.06409140676259995\n",
      "Step: 4330, Loss: 0.03798982873558998\n",
      "Step: 4340, Loss: 0.060587044805288315\n",
      "Step: 4350, Loss: 0.07742034643888474\n",
      "Step: 4360, Loss: 0.027487199753522873\n",
      "Step: 4370, Loss: 0.05880353972315788\n",
      "Step: 4380, Loss: 0.020087022334337234\n",
      "Step: 4390, Loss: 0.03820403292775154\n",
      "Step: 4400, Loss: 0.0449998676776886\n",
      "Step: 4410, Loss: 0.09426175802946091\n",
      "Step: 4420, Loss: 0.035615917295217514\n",
      "Step: 4430, Loss: 0.04876171424984932\n",
      "Step: 4440, Loss: 0.038589075207710266\n",
      "Step: 4450, Loss: 0.029091760516166687\n",
      "Step: 4460, Loss: 0.07491041719913483\n",
      "Step: 4470, Loss: 0.03667183592915535\n",
      "Step: 4480, Loss: 0.05198962613940239\n",
      "Step: 4490, Loss: 0.06448792666196823\n",
      "Step: 4500, Loss: 0.08010322600603104\n",
      "Step: 4510, Loss: 0.0362197682261467\n",
      "Step: 4520, Loss: 0.06760387867689133\n",
      "Step: 4530, Loss: 0.03507443889975548\n",
      "Step: 4540, Loss: 0.06388207525014877\n",
      "Step: 4550, Loss: 0.04830228537321091\n",
      "Step: 4560, Loss: 0.008217276073992252\n",
      "Step: 4570, Loss: 0.045784421265125275\n",
      "Step: 4580, Loss: 0.03188951313495636\n",
      "Step: 4590, Loss: 0.05281650647521019\n",
      "Step: 4600, Loss: 0.0411040335893631\n",
      "Step: 4610, Loss: 0.04510312154889107\n",
      "Step: 4620, Loss: 0.02349133789539337\n",
      "Step: 4630, Loss: 0.04037995636463165\n",
      "Step: 4640, Loss: 0.023133039474487305\n",
      "Step: 4650, Loss: 0.08327821642160416\n",
      "Step: 4660, Loss: 0.1145164892077446\n",
      "Step: 4670, Loss: 0.04311324283480644\n",
      "Step: 4680, Loss: 0.047478944063186646\n",
      "Step: 4690, Loss: 0.05042707175016403\n",
      "Step: 4700, Loss: 0.07461012899875641\n",
      "Step: 4710, Loss: 0.04567275568842888\n",
      "Step: 4720, Loss: 0.0406268909573555\n",
      "Step: 4730, Loss: 0.060204289853572845\n",
      "Step: 4740, Loss: 0.055535197257995605\n",
      "Step: 0, Loss: 0.08420369774103165\n",
      "Step: 10, Loss: 0.03749208152294159\n",
      "Step: 20, Loss: 0.027601925656199455\n",
      "Step: 30, Loss: 0.02737896889448166\n",
      "Step: 40, Loss: 0.006668965332210064\n",
      "Step: 50, Loss: 0.007210478652268648\n",
      "Step: 60, Loss: 0.0473872534930706\n",
      "Step: 70, Loss: 0.03345370665192604\n",
      "Step: 80, Loss: 0.04137222841382027\n",
      "Step: 90, Loss: 0.039210353046655655\n",
      "Step: 100, Loss: 0.05168955400586128\n",
      "Step: 110, Loss: 0.04307255148887634\n",
      "Step: 120, Loss: 0.03301836922764778\n",
      "Step: 130, Loss: 0.016954734921455383\n",
      "Step: 140, Loss: 0.03158878907561302\n",
      "Step: 150, Loss: 0.03767044469714165\n",
      "Step: 160, Loss: 0.024957705289125443\n",
      "Step: 170, Loss: 0.022097589448094368\n",
      "Step: 180, Loss: 0.021553872153162956\n",
      "Step: 190, Loss: 0.022799406200647354\n",
      "Step: 200, Loss: 0.05318712070584297\n",
      "Step: 210, Loss: 0.03820918872952461\n",
      "Step: 220, Loss: 0.02640780434012413\n",
      "Step: 230, Loss: 0.014429887756705284\n",
      "Step: 240, Loss: 0.0415567122399807\n",
      "Step: 250, Loss: 0.02647313103079796\n",
      "Step: 260, Loss: 0.01428812462836504\n",
      "Step: 270, Loss: 0.040945202112197876\n",
      "Step: 280, Loss: 0.07108031958341599\n",
      "Step: 290, Loss: 0.03261517360806465\n",
      "Step: 300, Loss: 0.030942382290959358\n",
      "Step: 310, Loss: 0.042754847556352615\n",
      "Step: 320, Loss: 0.050591204315423965\n",
      "Step: 330, Loss: 0.02207089588046074\n",
      "Step: 340, Loss: 0.02836700715124607\n",
      "Step: 350, Loss: 0.09052179008722305\n",
      "Step: 360, Loss: 0.012575333006680012\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 10\u001b[0m logits, pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     13\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m, in \u001b[0;36mFineTunedBert.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 13\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     out \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:524\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    516\u001b[0m         hidden_states,\n\u001b[1;32m    517\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m         output_attentions,\n\u001b[1;32m    523\u001b[0m     )\n\u001b[0;32m--> 524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:466\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 466\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    468\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp2/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        words, x, is_head, tags, y, seqlens = batch\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits, pred = model(x)\n",
    "\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "        y = y.view(-1)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print(f\"Step: {i}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  \n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        words, x, is_head, tags, y, seqlens = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits, pred = model(x)\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "        y = y.view(-1)\n",
    "\n",
    "        pred_labels = logits.argmax(dim=-1)\n",
    "\n",
    "        predictions.extend(pred_labels.cpu().numpy())\n",
    "        true_labels.extend(y.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9825\n",
      "Test Precision: 0.9832\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving model...\")\n",
    "torch.save(model, \"model/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(sentence, model, tokenizer: BertTokenizer, device, tag2idx, idx2tag):\n",
    "    \"\"\"\n",
    "    Predict the POS tag for a sentence using a fine-tuned BERT model.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    words = sentence.split()\n",
    "\n",
    "    inputs = tokenizer(words, \n",
    "                       return_tensors='pt', \n",
    "                       truncation=True,\n",
    "                       is_split_into_words=True,\n",
    "                       padding=True).to(device)\n",
    "    \n",
    "    x = inputs['input_ids']\n",
    "    x = x.to(device)\n",
    "\n",
    "    logits, _ =  model(x)\n",
    "    \n",
    "\n",
    "    logits = logits.view(-1, logits.shape[-1])\n",
    "    pred_labels = logits.argmax(dim=-1)\n",
    "\n",
    "    pred_labels = pred_labels.cpu().numpy()\n",
    "    \n",
    "    pad_idx = tag2idx[\"<pad>\"]\n",
    "\n",
    "    return [idx2tag[t] for t in pred_labels if t != pad_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O rato roeu a roupa do rei de Roma\n",
      "['ART', 'N', 'V', 'ART', 'N', 'PREP+ART', 'N', 'PREP', 'NPROP']\n"
     ]
    }
   ],
   "source": [
    "word = 'O rato roeu a roupa do rei de Roma'\n",
    "predicted_tag = predict_sentence(word, model, tokenizer, device, tag2idx, idx2tag)\n",
    "print(word)\n",
    "print(predicted_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
